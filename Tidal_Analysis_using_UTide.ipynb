{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"20%\" style=\"padding-right:10px;\" src=\"Images/Ccom.png\">\n",
    "\n",
    "# On Tides and Tidal Analysis\n",
    "\n",
    "\n",
    "\n",
    "### Adapted by Semme J. Dijkstra from a Lab by Andy Armstrong and Jim Irish 2020\n",
    "\n",
    "v2.0.0: Semme J. Dijkstra: Complete overhaul for more logical flow and simpler explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.fft import fft, fftfreq\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import pi, cos, sin\n",
    "from datetime import datetime, date, timedelta\n",
    "from utide import solve, reconstruct\n",
    "import matplotlib.dates as mdates\n",
    "import utide\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "tide_path=os.path.abspath(os.path.curdir)+\"/mydata/\"\n",
    "\n",
    "sys.path.append(str(tide_path)) # add the folder to the list of paths \n",
    "# print(sys.path)\n",
    "# from mycode.specter import specter\n",
    "\n",
    "print('This notebook uses UTide version'+utide.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Levels and Tidal Analysis Using Two Tide Gauges\n",
    "\n",
    "In this `Notebook` data is taken from two gauges that measure water levels by measuring pressure. A short introduction on the principle of such measurements is given, followed by a short introduction to tidal analysis. You will do some analysis using both **Fourier Transforms**, in particular **Fast Fourier Transforms (FFT)**., and the Least \n",
    "\n",
    "If you want to learn more in depth about tides you may consult the book [NOAA Special Publication NOS CO-OPS 3](Documents/Tidal_Analysis_and_Predictions.pdf) as it is an excellent resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Bottom Pressure Measurement\n",
    "\n",
    "In the case where the ocean is in approximate static equilibrium, the vertical equation of motion becomes the hydrostatic equation, and this can be integrated from the top of the atmosphere ($\\infty$) to the bottom of the ocean ($-h$) to obtain the pressure observed by the instrument deployed there. \n",
    "\n",
    "$$P_{-h}(t)=\\int_{-h}^{\\infty} \\rho(z,t) \\cdot g \\cdot dz$$\n",
    "\n",
    "We can break this up into several pieces as follows:\n",
    "\n",
    "$$P_{-h}(t)=\\int_{\\eta}^{\\infty} \\rho_{atm}(z,t) \\cdot g \\cdot dz + \n",
    "            \\int_{0}^{\\eta} \\rho_{0}(z,t) \\cdot g \\cdot dz + \n",
    "            \\int_{-h}^{0} \\rho_{avg}(z,t) \\cdot g \\cdot dz +\n",
    "            \\int_{-h}^{0} \\rho'(z,t) \\cdot g \\cdot dz$$\n",
    "            \n",
    "The integral from the top of the atmosphere ($z=\\infty$) to the sea surface ($z=\\eta$) is just the atmospheric pressure, $P_{atm}$. \n",
    "\n",
    "The sea surface is generally not smooth, but consists of waves whose elevation we describe by $\\eta$ as a function of time.  Thus the next term is the integral from the sea surface ($z=\\eta$) to the mean sea level ($z=0$).  This then is a pressure due to the surface waves that we approximate as the surface density, $\\rho_0$, times the elevation $\\eta$.  However, remember that surface wave effects decrease with depth as $e^{-k\\cdot z}$ where $k$ is the surface wave wavenumber ($k = 2\\pi/\\lambda$) and $z$ the depth.\n",
    "\n",
    "The next term involves the integral from the mean sea level ($z=0$) to the sea floor ($z=-h$).  We shall break this further up into terms depending on the density.  The first is the average density, $\\rho_{avg}$, times the mean depth $-h$ and is just the mean hydrostatic pressure seen by the bottom pressure sensor.\n",
    "\n",
    "The final term is the deviation from the mean density where\n",
    "\t\t\t $$\\rho(z) = \\rho_{avg} + \\rho’(z,t)$$\n",
    "             \n",
    "where the perturbation density $\\rho’$ contains the effects of internal density changes on the bottom pressure. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now write the pressure our bottom instrument sees as the contribution of these terms:\n",
    "\n",
    "$$P_{-h}(t)=P_{atm}(t)+\\rho_0(t)\\cdot g \\cdot\\eta (t) + \\rho_{avg}\\cdot g \\cdot h + \\int_{-h}^{0} \\rho'(z,t) \\cdot g \\cdot dz$$\n",
    "            \n",
    "So we see the signal we see at the bottom is due to: \n",
    "- the atmospheric pressure\n",
    "- the contribution due to the waves and other deviations of the sea level from its mean position\n",
    "- the average hydrostatic pressure due to the depth of the water\n",
    "- the contribution due to internal density changes.  Which in shallow water situations is small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving for the time varying sea level variation, $\\eta(t)$, as a function of the time varying density:\n",
    "\n",
    "$$\\eta(t) = \\frac{\\{P_{-h}(t) - P_{atm}(t)\\}}{\\rho_0(t)\\cdot g} - constant$$\n",
    "            \n",
    "The term in curly brackets is the differential pressure that tide gauge at Jackson Lab measures.  This can also be calculated from an absolute pressure sensor and a nearby atmospheric pressure sensor\n",
    "\n",
    "The correction $\\rho_0(t)\\cdot g$ is a time varying term and should be measured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\rho(t)\\cdot g$ Correction from MicroCAT at Jackson Estuarine Lab\n",
    "\n",
    "<img align=\"left\" width=\"70%\" style=\"padding-right:10px;\" src=\"Images/density_gravity_corrn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Sine Wave\n",
    "\n",
    "A sine wave is a continuous smooth periodic oscillation. Sine waves occur in tidal motion due to the periodic motion of the Earth and astronomical bodies. In tides we tend to use the $1/2 \\pi$ complement to the sine, known as the cosine\n",
    "\n",
    "$$\\eta(t) = A\\cdot cos(f\\cdot t + \\theta)$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- A: amplitude\n",
    "- f: frequency or the cycles per second\n",
    "- $\\theta$:  phase of cycle at $t=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 0 Line Spectra\n",
    "\n",
    "In the top plot generated by the code cell below you see a 1024 point sine wave signal with a 43 point cycle. The power spectrum of that signal is given by the bottom plot. In this case you might expect a single peak at exactly $f=1/43\\approx0.023$, however this is not the case, which is due to the limitations of the fft algorithm that relies on the system being continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "\n",
    "n = 1024\n",
    "s = int(n/2)\n",
    "t = np.arange(n)\n",
    "f = 1/43\n",
    "A = 1\n",
    "eta = A*sin(2*pi*f*t)  # Create a sinusoid\n",
    "plt.plot(eta)\n",
    "plt.title('Signal')\n",
    "plt.ylabel('Water Level in [m] →') \n",
    "plt.xlabel('Point number →') \n",
    "plt.show()\n",
    "plt.figure();     # Show the sinusoid\n",
    "Fk = fft(eta)/n   # The FFT of the signal\n",
    "nu = fftfreq(n,1) # The natural frequencies in the spectrum\n",
    "plt.plot(nu[0:s],np.absolute(Fk[0:s])**2) # Plot the power spectrum\n",
    "plt.title('Power Spectrum')\n",
    "plt.ylabel('Power')\n",
    "plt.xlabel('Frequency')\n",
    "plt.xlim(0,0.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above change the length of record by changing the number of samples and observe what happens to the signal and its spectrum; at the hand of your results answer the following question:\n",
    "\n",
    "    - q_0_0: How does increasing the number of samples help improve the quality of the transform.\n",
    "\n",
    "Increasing the number of samples by increasing the sampling rate also leads to similar effect i.e., the number of samples that determines the achievable quality of the FFT. Besides the answer to the question above, the following are effects of changing the sampling rate and duration.\n",
    "\n",
    "- 1. Lengthening the record lowers the influence of transient and random noise\n",
    "- 2. Increasing the sampling rate increases the Nyquist frequency, thus the ability to resolve high frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Line Spectra\n",
    "q_0_0 = 'Increasing the number of samples improves the quality of the transform through increasing the ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Astronomical Frequencies\n",
    "\n",
    "The tide generating forces are cyclical forces due to gravity acceleration of various astronomical processes. For tidal modeling there are 6 associated frequencies of particular importance \n",
    "\n",
    "$f_1$ = 1 cycle per lunar day = earth rotation (high moon to high moon) = 24.84 hours = 14.4921º/hr<br>\n",
    "$f_2$ = 1 cycle per month = moon around earth = 27.3216 mean solar days = 0.544º/hr<br>\n",
    "$f_3$ = 1 cycle per year = earth around sun = 365.2422 mean solar days = 0.0411º/hr<br>\n",
    "$f_4$ = 1 cycle per 8.85 Julian years (365.25 days) = lunar perigee<br>\n",
    "$f_5$ = 1 cycle per 18.61 Julian years = regression of the nodes<br>\n",
    "$f_6$ = 1 cycle per 20,900 years = solar perigee<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Tidal Frequencies\n",
    "\n",
    "To determine the tidal elevation $\\eta(t)$ we may use the expression:\n",
    "\n",
    "$$\\eta(t) = \\Sigma_k\\cdot A_k\\cdot cos(f_k\\cdot t + \\theta_k)$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $A_k$: amplitude of tidal constituent $k$\n",
    "- $f_k$: frequency of tidal constituent $k$\n",
    "- $\\theta_k$:  phase of tidal constituent $k$\n",
    "\n",
    "The tidal frequencies $f_k$ are combinations of the basic astronomical frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Darwin Symbol\n",
    "\n",
    "Darwin symbols are shortcuts to identifying a tidal frequencies e.g., $M1, O1, K1, N2, M2, S2, M4$.\n",
    "\n",
    "The principle tidal frequencies are identified by their Darwin Symbol which is related to the daily, monthly and yearly tides.\n",
    "\n",
    "The 8 and 18 year tides are  handled separately as will be discussed later. \n",
    "\n",
    "The number in a Darwin Symbol represents the species, or the number of cycles per day of that constituent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# A Word on the Tidal Phases\n",
    "\n",
    "In the discussion of the determination of the tidal frequencies from the tidal equation the phase $\\theta_k$ and amplitude $A_k$ at each constituent were used. You have to use care with the phase term $\\theta_k$ as it is dependent on the time reference frame used\n",
    "\n",
    "- $G$ = Greenwich Epoch = phase lag of the observed tide behind the passage of the sun or moon over Greenwich Meridian with time kept in UTC.  \n",
    "- $g$ = same as $G$ but uses local time rather than UTC.\n",
    "- $\\kappa$ = local Epoch = phase lag of the observed tide behind the passage of the sun or moon over the local meridian.\n",
    "    - Where $G = k + s\\cdot$*local West longitude*<br>\n",
    "      $s$ = species with 0 for long period, 1 for diurnal, 2 for semi-diurnal, etc.  \n",
    "      \n",
    "**For plotting the spatial structure of the tide, you want the Greenwich Epoch (G) rather than the local epoch!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Spectrum of Tidal Constituents (AKA Tide Lines)\n",
    "\n",
    "<img align=\"left\" width=\"70%\" style=\"padding-right:10px;\" src=\"Images/Hartman-and-Wenzel_tidal_spectrum.png\">\n",
    "\n",
    "Source: Hartmann T. and Wenzel H.G., 1995, The HW95 tidal potential\n",
    "catalogue. Geophys. Res. Lett, 22, 3553–3556.\n",
    "\n",
    "Note that it may be seen that many of the constituents are clustered in terms of frequency. This presents difficulty in their analysis as there may be significant correlation in the analysis of amplitudes and phases of different frequencies within such a cluster. It is therefore useful to calculate the correlation in the solution of the coefficients in the analysis. The method of the least squares inherently allows for the determination of such correlations, making it a suitable tool for the estimation of the constituent parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Considerations\n",
    "\n",
    "## Tides of the solid earth.  \n",
    "\n",
    "The surface of the earth rises and falls up to 30 cm due to the forcing of the sun and moon.  The earth tides are generally in equilibrium with the tidal potential. The Love numbers describe the elasticity of the earth in response to the gravitational forcing.  \n",
    "\n",
    "## Tides of the solid moon\n",
    "\n",
    "seismometers on the moon show that there are “moon” tides due to the gravitational attraction of the sun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Tidal Analysis Methods\n",
    "\n",
    "To do tidal analysis there are two basic approaches\n",
    "\n",
    "- 1 Harmonic Method – NOAA/NOS Original Method\n",
    "\n",
    "    - Fourier Transform at tidal frequencies\n",
    "    - Requires 1 month or 1 year records with no gaps.\n",
    "<br><br>\n",
    "- 2 Least Squares – UTide\n",
    "    Fits predicted tide to observed in least squares sense\n",
    "    - Records may have gaps\n",
    "    - Any length record, but longer than 1 year.\n",
    "    - Uncertainty of the estimated amplitude and phase for each constituent are provided\n",
    "    - appropriate constituents are automatically selected as a results of the analysis of covariances (correlations)\n",
    "\n",
    "### Relative Performance\n",
    "For very long, “clean” records, the results from these methods are statistically the same. It may be argued that the least squares method is preferable for longer duration series (>1 year) due to being less sensitive to problems and or gaps in the input data, as well as the ability to robustly estimate the uncertainties associated to the constituents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Methods 1 Harmonic Method – NOAA/NOS\n",
    "\n",
    "- First tidal analysis was a Fourier transform at tidal frequencies given by astronomy - done by hand. No need for a full transform. \n",
    "- The transform gives phase relative to the record start and it needs to be adjusted relative to astronomy.     \n",
    "- NOAA normally uses 1 month or 1 year record lengths to get around windowing problems.\n",
    "\n",
    "## Nodal Correction\n",
    "\n",
    "Analysis on less than one year of data can not resolve 8 and 18.6 year effects. \n",
    "Therefore, use Darwin Symbols like M2 to “name” the frequency, and adjust the amplitude and phase of a one year record for the 8 and 18.6 yr tides by:\n",
    "      $$\\eta(t) = \\Sigma a_k\\cdot A_k\\cdot cos(f_k t + \\theta_k+ b_k)$$\n",
    "      \n",
    "Where $a_k$ and $b_k$ are “nodal” corrections in amplitude and phase for a record < 1 year at the midpoint of the record.\n",
    "\n",
    "- Analysis is by Fourier transform at the tidal frequency.  The frequency is determined quite accurately by astronomy \n",
    "    - M2 = 1.93227361 Cycles Per Day (`cpd`)\n",
    "- Analysis is done on the largest of the constituents as determined by equilibrium theory (e.g. M2 = 0.90812, and O1 = 0.37689).\n",
    "- NOAA uses 25 constituents (including higher harmonics e.g. M4, M6) to make a normal analysis. (Other routines use up to 49 constituents.)   \n",
    "- For short records (e.g. 1 month), 5 constituents are analyzed and the rest of the constituents (up to 49) are inferred from equilibrium theory. \n",
    "- The analysis selects the part of the signal which is in constant amplitude and phase relation with the observed at a constituent's frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Nodal Correction\n",
    "\n",
    "The crux of tidal analysis is how to analyze for the tidal part of the water level signal without including “noise.”\n",
    "There is “noise” in all geophysical data, so any analysis must consider how to separate the tidal signal from the background noise i.e., determine the Signal to Noise Ratio (`SNR`).\n",
    "\n",
    "For a good prediction, you do not want to predict all the variance (non tidal energy) in the record, but just the tidal part. In this method removing the noise from the data is done by using the coefficients to 'predict' the tides for the duration of observations and adjust them until an optimal match is found. This effectively addresses some of the limitations associated to discrete Fourier Transforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Prediction\n",
    "\n",
    "Once you have analyzed a record for the harmonic constants (e.g., M2  0.389 m  006.1º) you can predict the tide for any other time for that place. It is important that you understand that classical tidal analysis and prediction are only valid for only one point in space i.e., the location at which the observations were made.\n",
    "\n",
    "Before the advent of modern digital computers prediction was done using a tide generating machine. Tide generating machines are mechanical devices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Video\n",
    "Video.from_file(os.path.join(os.getcwd(), \"Images\", \"tide_prediction_machine_video.m4v\"), width=400, height=400)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"30%\" style=\"padding-right:10px;\" src=\"Images/tide_prediction_machine.png\">\n",
    "\n",
    "\n",
    "If you have not done so already execute the code in the code cell above. As you can see the length of the arm $K_i$ determines the radius of a circle that each arm traverses. The position of each arm $K_i$ may be described by its angle, which is the phase angle of the associated tidal constituent. The rotational rate of each arm then determines the frequency or period. Thus to predict the tide using 7 components (like in the animation) you will need rotating arms $K_i, i\\in[1,7]$ connected to pulleys, each arm rotating at a rate proportional to the period $T_i$ of the tidal constituent (i.e., M2, S2, N2 K1, O1, P1, and K2). Note that you may accelerate (scale) time by applying a common scalar (> 1) to each of the rotation rates. Scaling the time like this will allow for the prediction of tides. \n",
    "\n",
    "The length of each arm $K_i$ represents the amplitude, and the initial angular position the starting phase.\n",
    "\n",
    "The video above from the [American Mathematical Society](https://www.ams.org/home/page) is created by a wonderful JAVA simulation which quite effectively [shows how an example of how these machines work](http://www.ams.org/publicoutreach/feature-column/fcarc-tidesiii3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in WW II there was one machine in England and one NOAA headquarters in Maryland. These machines were hidden during WWII as they were of key importance to supplying information for the Normandy invasion, and others. The Germans had their own tidal predicting machine.  \n",
    "\n",
    "The tide predicting machines were complex mechanical systems where the amplitude and phase for each constituent were set and machine summed the results for a prediction.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img align=\"left\" width=\"50%\" style=\"padding-right:10px;\" src=\"Images/noaa_tp_machine_1.png\">\n",
    "<img align=\"left\" width=\"50%\" style=\"padding-right:10px;\" src=\"Images/noaa_tp_machine_2.png\">\n",
    "<img align=\"left\" width=\"50%\" style=\"padding-right:10px;\" src=\"Images/noaa_tp_machine_3.png\">\n",
    "<img align=\"left\" width=\"50%\" style=\"padding-right:10px;\" src=\"Images/noaa_tp_machine_4.png\">\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "The machine shown here was designed by Rollin A. Harris and E.G. Fischer and constructed in the instrument shop of the U.S. Coast and Geodetic Survey in 1910; it replaced the older Ferrel Machine in 1912. \n",
    "\n",
    "The machine is about 11 feet long, 2 feet wide, and 6 feet high, and weighs approximately 2,500 pounds. The principal features are: \n",
    "- the supporting framework\n",
    "- a system of gearing by means of which shafts representing the different constituents are made to rotate with angular speeds proportional to the actual speeds of the constituents \n",
    "- a system of cranks and sliding frames for obtaining harmonic motion\n",
    "- summation chains connecting the individual constituents elements, by means of which the sums of the harmonic terms are transmitted to the recording devices\n",
    "- a system of dials and pointers for indicating in a convenient manner the height of the tide for successive instants of time and also the time of the high and low waters; \n",
    "- a tide curve or graphic representation of the tide automatically constructed by the machine. \n",
    "\n",
    "The machine was designed to take account of the following 37 constituents: \n",
    "\n",
    "$J1, K1, K2, L2, M1, M2, M3, M4, M6, M8, N2, 2N2, O1, OO1, P1, Q1, 2Q1, R2, S1, S2, S4, S6, T2, lambda2, µ2, nu2, rho1, MK, 2MK, MN, MS, 2SM, Mf, MSf, Mm, Sa, Ssa$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Analysis Method 2: Least Squares Estimation\n",
    "\n",
    "\n",
    "Since we do not have access to a tide machine we will use numerical analysis for the derivation of tidal constituents, as well as tidal prediction using these constituents. We will use the [Utide](http://www.po.gso.uri.edu/~codiga/utide/utide.htm) library, which contains algorithms for a least squares fitting of observations of water levels using the standard tidal constituents. Utide is a follow on to [T_TIDE](Documents/t_tide.pdf) which in turn was based on Fortran code by [Mike Foreman](Documents/foremanREP1978.pdf) of the Institute of Ocean Sciences, Sidney BC, Canada. There is a Python [T_Tide repository](https://github.com/moflaher/ttide_py) and [UTide repository](https://github.com/wesleybowman/UTide) available at [GitHub].Unfortunately there is no in-depth manual for the Python implementation, but Daniel L. Codiga wrote a [Utide manual](Documents/2011Codiga-UTide-Report.pdf) for the Matlab version of the library that is very helpful.\n",
    "\n",
    "Here we will use UTide for a least squares estimation of tidal amplitudes and phases of data sets collected in the vicinity of CCOM/JHC facilities.\n",
    "\n",
    "The first record that we will analyze is from data collected using a pressure gauge installed by CCOM/JHC students at the Jackson Laboratory located near Great Bay in the Piscataqua estuary.\n",
    "\n",
    "The second record that we will investigate was acquired and processed by the [NOAA CO-OPS](https://tidesandcurrents.noaa.gov/about_us.html) who are responsible fore the acquisition and analysis of [tides and currents](https://tidesandcurrents.noaa.gov/). The data that we will investigate is from a time period that matches that of the data collected at the Jackson Lab and that stems from a tide gauge also installed in the estuary, but near the entrance at the Fort Point US Coast Guard facility in New Castle NH (right across from where RV Gulf Surveyor is docked)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 1. Assignment: Analysis of Two Tide Gauge Records\n",
    "\n",
    "\n",
    "As part of this assignment you will be comparing the data collected by CCOM students at the UNH Jackson laboratory in 2011 to data collected at a NOAA CO-OPS tide gauge. \n",
    "\n",
    "The first task is to read the data collected at the Jackson lab. These data are contained in a file called *JacksonLab.txt* which is provided to you in the **mydata** folder in the same directory as this assignment.\n",
    "\n",
    "We will first read the data file using the [Python Data Analysis Library (pandas)](https://pandas.pydata.org/). If you have not worked with this library before it is worthwhile perusing. It is particularly useful for creating parsers for data files, which is what we will be using it for here.\n",
    "\n",
    "In the code cell below we will open a data file, holding data for the fortPoint tide gauge and have a peak at its contents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Jackson Lab, Durham, NH\n",
    "\n",
    "<img align=\"left\" width=\"70%\" style=\"padding-right:10px;\" src=\"Images/jackson_lab.png\">\n",
    "\n",
    "  <br><br> This gauge was commissioned as part of CCOM/JHCs educational program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 1.1 Read and Parse the Jackson Laboratory Data\n",
    "\n",
    "The gauge obtains pressure that is measured by sensor in the bottom Seacat (SBE-16Plus) about 1.5 meters above the bottom.  \n",
    "\n",
    "- Statistics of the record are:\n",
    "    - Maximum = 55.36 dbars\n",
    "    - Minimum = 50.10 dbars\n",
    "    - Mean = 52.4929 dbars\n",
    "    - Variance = 0.9172 dbars²  (energy in record) \n",
    "        - for a sine wave: $Variance = \\frac{1}{2}A^2$ or $Variance = \\Sigma(value-mean)^2$\n",
    "- Where 1 dbar is about 1 m of sea level\n",
    "\n",
    "\n",
    "### 1.1.0 Take a Peek at the Data\n",
    "\n",
    "In the code cell below we will open a data file holding data for the Jackson tide gauge and have a peak at its contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1.0 Take a Peek at the Data\n",
    "\n",
    "with open(os.path.join(tide_path, \"JacksonLab.txt\")) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(\"\".join(lines[:5]))\n",
    "print(\"...\")\n",
    "print(\"\".join(lines[-5:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will illustrate the power of pandas here by using it to read and parse the file. We will want to read the file into what pandas refers to as a DataFrame. \n",
    "\n",
    "<img align=\"left\" width=\"6%\" style=\"padding-right:10px;\" src=\"Images/info.png\">A pandas DataFrame is a: [Two-dimensional, size-mutable, potentially heterogeneous tabular data](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) frame. It has the nice property that the rows may be indexed by number or, if provided, another index e.g., time. Similarly columns may be indexed by a name (label). This is consistent with the idea that Python should be as close to natural language as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Parse the Data\n",
    "\n",
    "For our tidal analysis we will need to create a `pandas` `DataFrame` with at least a column `elev` indexed by `Datetime` objects. We will achieve this in two steps, first by reading the data into a `DataFrame` and then by manipulating that data frame to conform to our requirements.\n",
    "\n",
    "From the peak of the contents of Jackson Lab tide gauge we can learn how we need to read the data file using the `pandas` read_csv function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"date\", \"seconds\", \"elev\"]\n",
    "\n",
    "obs_jl = pd.read_csv(\n",
    "    os.path.join(tide_path, \"JacksonLab.txt\"),\n",
    "    header=0,       # Note that we could use the header to infer column names, we will not do this here\n",
    "    names=names,    # Explicitly name the columns\n",
    "    skipinitialspace=True,\n",
    "    delim_whitespace=False, # In this case the delimiter is a comma\n",
    "    na_values=\"9.990\" #This is a default value for missing data\n",
    ")\n",
    "\n",
    "# Show the first five lines of data\n",
    "obs_jl.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above quite a bit happened:\n",
    "- 1. Names for three columns of data were defined\n",
    "        \n",
    "        \"date\", \"seconds\", \"elev\"\n",
    "        <br>\n",
    "- 2. The `pandas` [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function for reading for comma separated file was invoked\n",
    "- 3. `read_csv` was used to open the data file using an OS indepent path to the data file\n",
    "- 4. `read_csv` was instructed that the header consists of 1 line (line 0)\n",
    "- 5. `read_csv` was instructed to name the columns of the `DataFrame`, overruling the names specified in the header\n",
    "- 6. `read_csv` was configured to skip spaces after the delimiter\n",
    "- 7. `read_csv` was informed what strings to recognize as NaN\n",
    "\n",
    "Finally the `DataFrame` object was instructed to render itself by invoking its name `obs_jl`, but only for the first records by calling the `head()` method with the argument `5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### 1.1.1.1 Replace the Index with Datetime objects and Remove the `date` and `seconds` Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.1.1.1 Replace the Index with Datetime objects and Remove the `date` and `seconds` Columns\n",
    "\n",
    "try: \n",
    "    # Create a time index using Datetimes\n",
    "    index = pd.to_datetime(obs_jl.date,format='%Y%m%d')+pd.to_timedelta(obs_jl.seconds,unit='s')\n",
    "    obs_jl.index = index\n",
    "\n",
    "    #  We really only need the times as date times and elevations, drop the other columns\n",
    "    obs_jl = obs_jl.drop(['date','seconds'], axis=1)\n",
    "    print(obs_jl.head(5))\n",
    "except:\n",
    "    print('Already time indexed the data frame')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is mostly taken to introduce you to some of the powerful ways of manipulating data that `pandas` allows for. Note that when working in notebooks you should be careful with removing data columns. You can see this if you execute the code cell above twice: The first time you will be rewarded with a print of the first 5 rows of `obs_jl`, any subsequent time you will see the line 'Already time indexed the data frame'.\n",
    "\n",
    "What do you think would have happened had we not made use of a `try-except` block and executed the cell more than once?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### 1.1.2 Visualize the Data\n",
    "\n",
    "The code cell below will create a plot with two subplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1.2 Plot the Jackson Laboratory Data\n",
    "\n",
    "# Plot a single day - the first full day will be the 2nd day\n",
    "day_1_start = np.where( obs_jl.index.date == obs_jl.index.date[0])[0][-1]+1\n",
    "day_1_end = np.where( obs_jl.index.date == obs_jl.index.date[day_1_start])[0][-1]\n",
    "\n",
    "# Determine the date of the first full day\n",
    "\n",
    "day_1_date = obs_jl.index.date[day_1_start]\n",
    "\n",
    "fig, (ax0) = plt.subplots(figsize=(17, 5), nrows=1, sharey=True, sharex=False)\n",
    "\n",
    "\n",
    "# Plot the first day's worth of elevation data\n",
    "ax0.plot(obs_jl.index[day_1_start:day_1_end], obs_jl.elev[day_1_start:day_1_end], label=\"Observations\", color=\"C0\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title('Jackson Laboratory CCOM/JHC Tide Gauge Water Level: '+str(day_1_date))\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date+Time')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# Plot the entire data set\n",
    "fig, (ax1) = plt.subplots(figsize=(17, 5), nrows=1, sharey=True, sharex=False)\n",
    "ax1.plot(obs_jl.index, obs_jl.elev, label=\"Observations\", color=\"C1\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title('Jackson Laboratory CCOM/JHC Tide Gauge Water Level, Complete Record')\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date')\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first subplot contains the data for the first full day of observed water level data, the date of which is contained in the variable `day_1_date`, the second is the full time series of water level observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Fort point, New Castle, NH\n",
    "\n",
    "<img align=\"left\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/fort_point_1.png\">\n",
    "<img align=\"left\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/fort_point_2.png\">\n",
    "\n",
    "  <br><br> This gauge was decommissioned in 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 1.2 Read and Parse NOAA CO-OPS Water Level Data\n",
    "\n",
    "In this section you are asked to get water level data for a period containing the epoch of the Jackson Laboratory data contained in the `obs_jl` `DataFrame`. The water level data will need to be from the gauge installed nearest to the one at the Jackson Laboratory (Adams Point), which at that time was located at Fort Point, at the US Coast Guard facility opposite the UNH pier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___\n",
    "### 1.2.2 Learn about the Fort Point Station\n",
    "\n",
    "Use the tides and currents [map page](https://tidesandcurrents.noaa.gov/map/) to find the station data of the Fort Point Water Level gauge. Note that you will have to select 'Historic Data' for the **Data Type** (You are responsible for finding that option) in order for the station to show up in the map.\n",
    "\n",
    "Answer the following questions\n",
    "\n",
    "- q_1_2_2_0. When was the station established?\n",
    "- q_1_2_2_1. What is the time meridian associated to this station?\n",
    "- q_1_2_2_2. When was the station removed?\n",
    "- q_1_2_2_3. What is the latitude of the station in ***decimal degrees with 4 significant figures***\n",
    "- q_1_2_2_4. What is the station ID?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_2_2_0 = 'The station was established on: ...'\n",
    "q_1_2_2_1 = 'The time meridian associated to this station ...'\n",
    "q_1_2_2_2 = 'The station was decommissioned on: ...'\n",
    "q_1_2_2_3 = 'The latitude of the station is: ... N'\n",
    "q_1_2_2_4 = 'The station ID is: ... '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### 1.2.3 Identify Other Historic Water Level Station in the Vicinity\n",
    "\n",
    "Use the tides and currents [map page](https://tidesandcurrents.noaa.gov/map/) to find the station ids of all the stations between (and including) Fort Point and Adams point that observed *water levels* at any point in history\n",
    "\n",
    "Assign the answer as a list of integers to the variable `q_1_2_3_0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_1_2_3_0 = [...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### 1.2.4 Dowload Water Level for the Jackson Laboratory Epoch\n",
    "\n",
    "We will now download the data for the same epoch as for which we have data at the CCOM JHC Jackson Laboratory data\n",
    "\n",
    "Use the tides and currents [map page](https://tidesandcurrents.noaa.gov/map/) to select the Fort Point station. \n",
    "\n",
    "Click on the `Station Home` button and then select Water Levels from the Tides/Water Levels drop down menu.\n",
    "\n",
    "Specify the options for the Fort Point download to span the dates from the first record to the last record of the Jackson Laboratory data, using Metric units, the GMT Timezone, the MLLW Datum, at an 1 hr interval. \n",
    "\n",
    "- 1.2.4.0 Retrieve the data\n",
    "- 1.2.4.1 Use the Export to CSV option\n",
    "- 1.2.4.1 Rename the data downloaded data file 'FortPoint.csv'\n",
    "- 1.2.4.2 Upload the 'FortPoint.csv' to the `mydata` folder of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output:\n",
    "<img align=\"center\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/1_2_4_FP_CSV_File.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Take a Peek at the Data\n",
    "\n",
    "Take a peak at the data in the 'FortPoint.csv' file in the same manner as you did for the Jackson Laboratiry data in step `1.1.0`. Update the code cell below to achieve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.5 Take a Peek at the Data\n",
    "\n",
    "with open(os.path.join(...)) as f:\n",
    "    ...\n",
    "\n",
    "print(\"\".join(lines[:5]))\n",
    "print(\"...\")\n",
    "print(\"\".join(lines[-5:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "    \"Date\",\"Time (GMT)\",\"Predicted (m)\",\"Preliminary (m)\",\"Verified (m)\"\n",
    "    \"2010/09/23\",\"00:00\",\"1.148\",\"-\",\"1.147\"\n",
    "    \"2010/09/23\",\"01:00\",\"1.805\",\"-\",\"1.796\"\n",
    "    \"2010/09/23\",\"02:00\",\"2.361\",\"-\",\"2.346\"\n",
    "    \"2010/09/23\",\"03:00\",\"2.682\",\"-\",\"2.712\"\n",
    "\n",
    "    ...\n",
    "    \"2011/01/01\",\"19:00\",\"-0.083\",\"-\",\"0.023\"\n",
    "    \"2011/01/01\",\"20:00\",\"-0.157\",\"-\",\"-0.065\"\n",
    "    \"2011/01/01\",\"21:00\",\"0.122\",\"-\",\"0.225\"\n",
    "    \"2011/01/01\",\"22:00\",\"0.676\",\"-\",\"0.794\"\n",
    "    \"2011/01/01\",\"23:00\",\"1.358\",\"-\",\"1.5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Parse the Data\n",
    "\n",
    "We want to parse this data file into a `DataFrame` `obs_fp` in similar manner to how we read the Jackson Lab data into `obs_fp` in step `1.1.1`.\n",
    "\n",
    "However the data structure is somewhat different. In this case use `pd.read_csv()` to read the file `FortPoint.csv` in the `DataFrame` `obs_fp`. Use the `header` to name the columns i.e., do not set the `names` argument like you did in step `1.1.1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.5 Parse the Data\n",
    "\n",
    "obs_fp = pd.read_csv(\n",
    "    os.path.join(tide_path, 'FortPoint.csv'),\n",
    "    header= ...,       \n",
    "    skipinitialspace=True,\n",
    "    delim_whitespace=..., # In this case the delimiter is a comma\n",
    "    na_values=\"9.990\" #This is a default value for missing data\n",
    ")\n",
    "\n",
    "# Show the first five lines of data\n",
    "obs_fp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output (without the nice formatting):\n",
    "\n",
    " \tDate \tTime (GMT) \tPredicted (m) \tPreliminary (m) \tVerified (m)\n",
    "    0 \t2010/09/23 \t00:00 \t1.148 \t- \t1.147\n",
    "    1 \t2010/09/23 \t01:00 \t1.805 \t- \t1.796\n",
    "    2 \t2010/09/23 \t02:00 \t2.361 \t- \t2.346\n",
    "    3 \t2010/09/23 \t03:00 \t2.682 \t- \t2.712\n",
    "    4 \t2010/09/23 \t04:00 \t2.707 \t- \t2.742"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### 1.2.6 Replace the Index with Datetime objects and Remove Unnecessary  Columns\n",
    "\n",
    "Use step 1.1.1.1 as an inspiration. You may run into an issue with the column labeled `Time (GMT)`. An alternative way of identifying this column is: `obs_fp['Time (GMT)']`. Also, it is easier to combine the `Date` and `Time (GMT)` columns and using the result in a call to `pandas.to_datetime()`, rather than using the sum of a `pandas.to_datetime()` and a `pandas.to_timedelta()` call.\n",
    "\n",
    "You may combine the columns using:\n",
    "\n",
    "    t = obs_fp.Date+' '+obs_fp['Time (GMT)']\n",
    "    \n",
    "and then determine the time index using\n",
    "\n",
    "    index = pd.to_datetime(t,format='...')\n",
    "    \n",
    "Where you will have to replace the ellipsis by the proper format string.\n",
    "\n",
    "Next up you will need to remove all but the `Verified (m)` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.6 Replace the Index with Datetime objects and and Remove Unnecessary Columns\n",
    "\n",
    "try: \n",
    "    # Create a time index using Datetimes\n",
    "    t = obs_fp.Date+' '+...\n",
    "    index = pd.to_datetime(t,format='...')\n",
    "    obs_fp.index = index\n",
    "\n",
    "    #  We really only need the times as date times and Verified elevations, drop the other columns\n",
    "    obs_fp = obs_fp.drop([...], axis=1)\n",
    "\n",
    "except:\n",
    "    print('Already time indexed the data frame')\n",
    "\n",
    "print(obs_fp.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output:\n",
    "\n",
    "                         Verified (m)\n",
    "    2010-09-23 00:00:00         1.147\n",
    "    2010-09-23 01:00:00         1.796\n",
    "    2010-09-23 02:00:00         2.346\n",
    "    2010-09-23 03:00:00         2.712\n",
    "    2010-09-23 04:00:00         2.742\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### 1.2.7 Rename the `Verified (m) ` Column to 'elev' \n",
    "\n",
    "To complete the parsing rename the `Verified (m)` column of the `obs_fp` `DataFrame` to 'elev'. Check the `pandas` documentation to find the method for renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.7 Rename the `Verified (m) ` Column to 'elev' \n",
    "\n",
    "obs_fp = obs_fp....(columns={'...:'...'} )\n",
    "print(obs_fp.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output:\n",
    "\n",
    "                          elev\n",
    "    2010-09-23 00:00:00  1.147\n",
    "    2010-09-23 01:00:00  1.796\n",
    "    2010-09-23 02:00:00  2.346\n",
    "    2010-09-23 03:00:00  2.712\n",
    "    2010-09-23 04:00:00  2.742\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.8 Plot the Fort Point Data\n",
    "\n",
    "The Fort Point data is now parsed, it is contained in the `obs_fp` `DataFrame`. This step will visualize the data for the same date as in step 1.1.2. Note that the date is already identified and contained in the variable `obs_fp`. This simplifies the determination of the indexes `day_1_start` and `day_1_end`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.8 Plot the Jackson Laboratory Data\n",
    "\n",
    "# Plot a single day - the first full day will be the 2nd day\n",
    "day_1_start = np.where( obs_fp.index.date == day_1_date)[0][0]\n",
    "day_1_end = np.where( obs_fp.index.date == day_1_date)[0][-1]\n",
    "\n",
    "fig, (ax0) = plt.subplots(figsize=(17, 5), nrows=1, sharey=True, sharex=False)\n",
    "\n",
    "# Plot the first day's worth of elevation data\n",
    "ax0.plot(obs_fp.index[day_1_start:day_1_end], obs_fp.elev[...], label=\"Observations\", color=\"C0\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title('Fort Point Tide Gauge Water Level: '+str(day_1_date))\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date+Time')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# Plot the entire data set\n",
    "fig, (ax1) = plt.subplots(figsize=(17, 5), nrows=1, sharey=True, sharex=False)\n",
    "ax1.plot(..., ..., label=\"Observations\", color=\"C1\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title('Fort Point Tide Gauge Water Level, Complete Record')\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date')\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output: \n",
    "<img align=\"center\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/1_2_8_FP_Water_Levels.png\"><img align=\"center\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/1_2_8_FP_Water_Levels_Full.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Analysis\n",
    "\n",
    "We will initially look at the data in a very simple fashion by plotting the first full day of both time series and performing some simple analysis. Though that will allow you to get a basic feeling for the data, it will not allow for the determination of tidal constituents. Therefore we will complete an analysis using the method of the least squares as implemented in the `UTide` python module. However, this module is to be seen as an introduction to working with tides and not an-depth treatise. We will therefore not address the various `Utide` options in great detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2.1 Basic Analysis\n",
    "\n",
    "Start by plotting the data of a full day contained in both time series. We already have the variable `day_1_date` containing such a date, so we will use this. \n",
    "\n",
    "### 2.1.1 Sub-Setting Data at Common Epochs\n",
    "\n",
    "The problem is that the data for the Fort Point station are at 1 hour interval, whereas the Jackson Lab data are at 6 minute intervals. We could overcome this by interpolating the data, but since there are data at common points in time this is unnecessarry. We may make convenient use of the indexes of the `DataFrames` to simply subset the data sets by using the following:\n",
    "\n",
    "    day_1_fp_dates = obs_fp.index[obs_fp.index.date == day_1_date]\n",
    "    day_1_fp_indexes = np.where(obs_fp.index.isin(day_1_fp_dates))[0]\n",
    "    day_1_jl_indexes = np.where(obs_jl.index.isin(day_1_fp_dates))[0]\n",
    "\n",
    "Convince yourself that you understand what the above code snippet does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1.1 Sub-Setting Data at Common Epochs\n",
    "\n",
    "day_1_fp_dates = obs_fp.index[obs_fp.index.date == day_1_date]\n",
    "day_1_fp_indexes = np.where(obs_fp.index.isin(day_1_fp_dates))[0]\n",
    "day_1_jl_indexes = np.where(obs_jl.index.isin(day_1_fp_dates))[0]\n",
    "\n",
    "print(obs_fp.index[day_1_fp_indexes][0:5])\n",
    "print(obs_jl.index[day_1_jl_indexes][0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### 2.1.2 Plotting Data at Common Epochs\n",
    "\n",
    "We may now plot the data for the first full day of the Fort Point data at the common epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.2 Plotting Data at Common Epochs\n",
    "\n",
    "fig, (ax0) = plt.subplots(figsize=(17, 5), nrows=1, sharey=True, sharex=False)\n",
    "\n",
    "# Plot the first day's worth of elevation data\n",
    "ax0.plot(obs_fp.index[day_1_fp_indexes], obs_fp.elev[...], label=\"Fort Point\", color=\"C0\")\n",
    "ax0.plot(obs_jl.index[...], ..., label=\"Jackson Lab\", color=\"C1\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title('Observed Water Levels: '+str(day_1_date))\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date+Time')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# Plot the entire data sets\n",
    "\n",
    "fig, (ax1) = plt.subplots(figsize=(17, 7), nrows=1, sharey=True, sharex=False)\n",
    "ax1.plot(obs_fp.index, obs_fp.elev, label=\"Fort Point\", color=\"C0\")\n",
    "ax1.plot(obs_jl...., ..., label=\"Jackson Lab\", color=\"C1\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title('Observed Water Levels: Complete Record')\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date')\n",
    "fig.legend(ncol=3, loc=\"lower center\")\n",
    "plt.gcf().autofmt_xdate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output:\n",
    "<img align=\"center\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/2_1_2FP_JL_Data_Common_Epoch.png\">\n",
    "<img align=\"center\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/2_1_2FP_JL_Data_Common_Epoch_Full.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lower plot it is obvious that the water levels at both stations act similarly, though the range at Jackson Lab is attenuated. It is also clear that there is a significant delay with respect to the Fort Point station that is on the order of 2 hours. Comparing the data is complicated by the fact that they are not on the same datum (Fort Point is relative to MLLW, the Jackson Lab data is on a datum relative to the gauge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Water Levels as Anomalies\n",
    "\n",
    "To address the fact that data are not on a common datum has the potential to complicate the analysis of the data. It is therefore common to interpret the water levels as anomalies i.e., to determine a mean water level (MSL) for the time series and offsetting all the observations by this mean.\n",
    "\n",
    "You may add a column to the `DataFrame` in a similar way to how this is done with a `dict` i.e., to add a column `anomaly` we may use:\n",
    "\n",
    "    obs_jl['anomaly'] = ...\n",
    "    \n",
    "In the code cell below add the column to both `obs_jl` and `obs_fp`. Make sure that the columns contain the water levels as anomalies, use the `mean()` method for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_jl['anomaly'] = obs_jl[\"elev\"] - obs_jl[\"elev\"].mean()\n",
    "obs_fp['anomaly'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.3 Plot the first full data\n",
    "\n",
    "fig, (ax0) = plt.subplots(figsize=(17, 5), nrows=1, sharey=True, sharex=False)\n",
    "\n",
    "# Plot the first day's worth of elevation data\n",
    "ax0.plot(obs_fp.index[day_1_fp_indexes], obs_fp.anomaly[day_1_fp_indexes], label=\"Fort Point\", color=\"C0\")\n",
    "ax0.plot(obs_jl.index[...], ..., label=\"Jackson Lab\", color=\"C1\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title('Observed Water Levels: '+str(day_1_date))\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date+Time')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# Plot the entire data sets\n",
    "\n",
    "fig, (ax1) = plt.subplots(figsize=(17, 5), nrows=1, sharey=True, sharex=False)\n",
    "ax1.plot(obs_fp.index, obs_fp.anomaly, label=\"Fort Point\", color=\"C0\")\n",
    "ax1.plot(obs_jl.index, obs_jl.anomaly, label=\"Jackson Lab\", color=\"C1\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title('Observed Water Levels: Complete Record')\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date')\n",
    "fig.legend(ncol=3, loc=\"lower center\")\n",
    "plt.gcf().autofmt_xdate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output: \n",
    "<img align=\"center\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/2_1_3_FP_JL_Data_Anomaly_Common_Epoch.png\">\n",
    "<img align=\"center\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/2_1_3_FP_JL_Data_Anomaly_Common_Epoch_Full.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Analysis Using Fast Fourier Transforms\n",
    "\n",
    "We will take the analysis a small step further by looking at the spectrum of the data. This allows us to get an estimate of the amplitude associated to the various harmonic constituents. In the code cell below you will see how the spectrum of the water level for the Fort Point tides is derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spectrum using a FFT\n",
    "\n",
    "df = obs_fp\n",
    "\n",
    "X = fft(df['anomaly'])\n",
    "N = len(df)\n",
    "n = np.arange(N)\n",
    "dt = (df.index[1]-df.index[0]).total_seconds()\n",
    "sr = 1 / dt\n",
    "T = N/sr\n",
    "freq = n/T \n",
    "\n",
    "# Get the one-sided specturm\n",
    "\n",
    "n_oneside = N//2\n",
    "\n",
    "# get the one sided frequency range\n",
    "f_oneside = freq[:n_oneside]\n",
    "\n",
    "# Get the one sided period range\n",
    "t_h = 1/f_oneside[1:] / dt\n",
    "\n",
    "# Plot the spectrum\n",
    "fig, (ax0, ax1) = plt.subplots(figsize=(17, 12), nrows=2, sharey=False, sharex=False)\n",
    "ax0.plot(f_oneside, np.abs(X[:n_oneside]), 'C0')\n",
    "ax0.set_title('Spectogram of Fort Point Tides')\n",
    "ax0.set(xlabel='Freq [Hz]')\n",
    "ax0.set(ylabel='FFT Amplitude |X(freq)|')\n",
    "\n",
    "# Plot the periodogram\n",
    "ax1.plot(t_h, np.abs(X[1:n_oneside])/(n_oneside-1),'C1')\n",
    "ax1.set_title('Periodogram of Fort Point Tides')\n",
    "ax1.set(ylabel='Amplitude [m]')\n",
    "ax1.set(xlabel='Period [Hour]')\n",
    "ax1.set_xlim(0, 100)\n",
    "ax1.set_xticks([12, 24 ,48, 96])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As can be seen from the code cell above the amplitude associated to the various constituents frequencies or periods may be determined. However, using this simple transform is not without its problems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of FFT problems\n",
    "\n",
    "# Find the index associated to the maximum amplitude in the spectrum\n",
    "max_f = np.argmax(np.abs(X[1:n_oneside]))\n",
    "\n",
    "# Find the associated period\n",
    "t_max = t_h[max_f]\n",
    "\n",
    "print('The maximum is associated to period: %.3f hours'%t_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above you see that the maximum response in the tidal spectrum is associated to a period line of 12.431 hours. This period matches none of the tidal constituents, but is close to that of several of the semi diurnal tides such as $M2$, $N2$, and $S2$. This is due to the fact that the Fast Fourier Transform has a frequency and period resolution that is determined by the number of samples; to ensure that all the constituents are covered exactly therefore requires significantly high sampling rates and/or long epochs of records. \n",
    "\n",
    "From this limited resolution it is easy to understand that significant correlation in the analysis of amplitudes and phases of nearby frequencies may occur. Though steps may be taken to get better results with Fourier Transforms using the approaches described [here](Documents/Tidal_Analysis_and_Predictions.pdf) most organizations outside NOAA CO-OPS use the **least squares method (LSM or MLS)** and/or a wavelet transform. It should be noted that the performance of either these approaches is still dependent on sampling rate and the length of the record - if the record is sufficiently long there will be no statistically significant difference between the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2.2 Analysis Using the Method of Least Squares\n",
    "\n",
    "For MLS we can fit data to specific pre-determined frequencies associated to the tidal constituents, thereby reducing the correlation between frequency adjacent tidal constituents due to insufficient resolution; this is the approach taken in `t_tide` and `u_tide`. Also, the method of the least squares inherently allows for the explicit estimation of the correlation between tidal constituent estimates, making it a suitable tool for tidal analysis.\n",
    "\n",
    "Due to the tidal frequencies being fixed inputs MLS analysis does not rely on a evenly spaced sampling interval. Thus reconstruction of tides from observations of opportunity is possible. For example data collected from a ferry moving along a transect may be used for any location along the transect (assuming that there are enough observations for that location).\n",
    "\n",
    "For those knowledgeable of MLS: we will make use of ordinary least squares analysis instead of iteratively reweighed least squares (IRLS). IRLS has some computational advantages, but requires a thorough understanding of the method of the least squares. \n",
    "\n",
    "We will also make use of automatically selected constituents, chosen on the likelihood of them being independent of each other. We will use linear confidence intervals as determined by the least squares method; an alternative would be to use the `'MC'` option which uses a Monte-Carlo simulation to predict different outcomes and determine the confidence intervals associated to them. This Notebook is written with the expectation that you do not know  how Monte-Carlo simulation works, and a thorough explanation is outside the scope of this assignment; the `MC` option would lead to a better estimate the covariances and thus lead to marginally better results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2.2.1 `Utide` Analysis of Jackson Laboratory Water Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_jl = utide.solve(\n",
    "    obs_jl.index,\n",
    "    obs_jl[\"anomaly\"],\n",
    "    lat=43.07,\n",
    "    method=\"ols\",\n",
    "    conf_int=\"linear\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above calls the `Utide` solve method which is used to estimate the tidal coefficients. This is achieved by using MLS to fit all possible constituents. A subset of constituents for which good results may be obtained is then selected automatically. Note that it is to be expected that there is an up- or downward trend in the data due to the relatively short duration, to not bias the results of teh analysis the data are de-trended by default. Also, the phase angles are calculated with respect to the Greenwhich Meridian passage by  default, meaning that the estimated phases are valid for a UTC time base.\n",
    "\n",
    "The code cell below takes the results from the `Utide` analysis and shows them in a format similar (not the same!) to what is shown for the **Station Home** page for the NOAA CO-OPS gauges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Darwin':>9}\"f\"{'T [h]':>9}\", f\"{'Amp':>6}\", f\"{'95%ci':>7}\",\n",
    "      f\"{'phase':>8}\", f\"{'95%ci':>9}\", f\"{'Energy %':>9}\", f\"{'SNR':>9}\")\n",
    "for i in range(len(coef_jl['name'])):\n",
    "    print('% 9s%9.3f%7.2f%8.3f%9.1f%10.1f%10.3f%10.2f' % (coef_jl.name[i],\n",
    "                                                          1/coef_jl.aux.frq[i],\n",
    "                                                          coef_jl.A[i],\n",
    "                                                          coef_jl.A_ci[i],\n",
    "                                                          coef_jl.g[i],\n",
    "                                                          coef_jl.g_ci[i],\n",
    "                                                          coef_jl.PE[i], \n",
    "                                                          coef_jl.SNR[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the table above to answer the following questions:\n",
    "- q_2_2_1_1: Explain what determines the level of significance for the tidal constituents.\n",
    "- q_2_2_1_2: Explain whether errors in the estimation of anything but the 6 most significant coefficients listed above makes much of a difference for tide prediction at this particular location if the record were decades long instead.\n",
    "- q_2_2_1_3: From the data seen above can you make a general statement about how many coefficients are minimally needed to reconstruct 99.5% of this tidal height signal?\n",
    "- q_2_2_1_4: Select the M2 constituent to estimate the time difference at which you would expect the same tidal phase at both locations. Explain how you calculated your estimate\n",
    "- q_2_2_1_5: In the table the `95%ci` and `SNR` are both indicators of data quality, which do you think is better indicator of potential bias?\n",
    "- q_2_2_1_6: When predicting tide, which combination for a particular constituent's values do you think affects the result the worst:\n",
    "    - a. low `Amp` with low `SNR`\n",
    "    - b. low `Amp` with high `SNR`\n",
    "    - c. high `Amp` with low `SNR`\n",
    "    - d. high `Amp` with high `SNR`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.1\n",
    "q_2_2_1_1 = 'The level of significance for the tidal constituents is determined by ...'\n",
    "q_2_2_1_2 = 'Using more than the 6 most significant listed constituents above  would make a ...'\n",
    "q_2_2_1_3 = 'From the data seen above you would need ... constituents to reconstruct 99.5% of the signal'\n",
    "q_2_2_1_4 = ' I estimate the time difference for the same tidal phase to be ... hours. I calculated this by'\n",
    "q_2_2_1_5 = 'Between the `95%ci` and `SNR` I think ... is the better indicator of potential bias'\n",
    "q_2_2_1_5 = 'a, b, c, or d'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2.2.2 `Utide` Analysis of Fort Point Water Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2\n",
    "\n",
    "coef_fp = utide.solve(\n",
    "    obs_fp.index,\n",
    "    ...,\n",
    "    ...,\n",
    "    ...,\n",
    "    ...,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the code cell above so that the tidal constituents for the Fort Point station are estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Darwin':>9}\"f\"{'T [h]':>9}\", f\"{'Amp':>6}\", f\"{'95%ci':>7}\",\n",
    "      f\"{'phase':>8}\", f\"{'95%ci':>9}\", f\"{'Energy %':>9}\", f\"{'SNR':>9}\")\n",
    "for i in range(len(coef_fp['name'])):\n",
    "    print('% 9s%9.3f%7.2f%8.3f%9.1f%10.1f%10.3f%10.2f' % (coef_fp.name[i],\n",
    "                                                          1/coef_fp.aux.frq[i],\n",
    "                                                          coef_fp.A[i],\n",
    "                                                          coef_fp.A_ci[i],\n",
    "                                                          coef_fp.g[i],\n",
    "                                                          coef_fp.g_ci[i],\n",
    "                                                          coef_fp.PE[i], \n",
    "                                                          coef_fp.SNR[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output:\n",
    "\n",
    "    Darwin    T [h]    Amp   95%ci    phase     95%ci  Energy %       SNR\n",
    "           M2   12.421   1.30   0.009    105.7       0.4    91.327  89207.86\n",
    "           N2   12.658   0.29   0.009     67.6       1.7     4.414   4314.54\n",
    "           S2   12.000   0.19   0.009    128.4       2.6     1.879   1836.16\n",
    "           K1   23.934   0.15   0.006    192.3       2.4     1.236   2129.45\n",
    "           O1   25.819   0.11   0.006    181.7       3.5     0.607   1045.77\n",
    "           L2   12.192   0.08   0.009    159.8       6.0     0.355    346.94\n",
    "           MM  661.309   0.03   0.030    108.6      56.1     0.050      4.02\n",
    "          MSF  354.367   0.03   0.030    117.0      59.4     0.044      3.59\n",
    "          MU2   12.872   0.02   0.009    157.2      23.1     0.024     23.68\n",
    "           M4    6.210   0.02   0.001    325.6       3.6     0.014    985.09\n",
    "           J1   23.098   0.02   0.006    222.7      23.5     0.013     22.82\n",
    "           Q1   26.868   0.01   0.006    137.8      26.6     0.010     17.86\n",
    "           M6    4.140   0.01   0.001    147.6       6.0     0.005    354.14\n",
    "          NO1   24.833   0.01   0.006    220.7      39.6     0.005      8.04\n",
    "         UPS1   21.578   0.01   0.006    100.7      48.3     0.003      5.40\n",
    "          MN4    6.269   0.01   0.001    295.5       9.7     0.002    134.34\n",
    "         2MN6    4.166   0.01   0.001    104.4      10.1     0.002    122.47\n",
    "          2Q1   28.006   0.01   0.006      0.9      68.7     0.002      2.67\n",
    "          MO3    8.386   0.00   0.001    227.6       8.9     0.001    159.89\n",
    "          MK3    8.177   0.00   0.001    231.7      10.4     0.001    117.09\n",
    "          MS4    6.103   0.00   0.001      4.7      14.1     0.001     63.92\n",
    "          OO1   22.306   0.00   0.006    300.9      90.4     0.001      1.54\n",
    "         EPS2   13.127   0.00   0.009     89.9     142.7     0.001      0.62\n",
    "         2MS6    4.092   0.00   0.001    167.6      17.4     0.001     41.72\n",
    "           M3    8.280   0.00   0.001    119.2      16.7     0.000     45.34\n",
    "         ALP1   29.073   0.00   0.006     89.0     154.7     0.000      0.53\n",
    "         ETA2   11.755   0.00   0.009    106.9     211.1     0.000      0.28\n",
    "          SN4    6.160   0.00   0.001     34.2      25.0     0.000     20.25\n",
    "         2MK5    4.931   0.00   0.001    108.2      16.6     0.000     45.52\n",
    "           M8    3.105   0.00   0.001    295.9      17.3     0.000     41.92\n",
    "          SK3    7.993   0.00   0.001    332.4      28.1     0.000     15.96\n",
    "         2SK5    4.797   0.00   0.001    253.4      24.2     0.000     21.57\n",
    "         3MK7    3.530   0.00   0.001    301.8      39.9     0.000      7.93\n",
    "           S4    6.000   0.00   0.001      5.8      87.3     0.000      1.66\n",
    "         2SM6    4.046   0.00   0.001    229.1     120.1     0.000      0.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the station data for the Fort Point tide gauge at the [CO-OPS map](https://tidesandcurrents.noaa.gov/map/) and compare the published values for the Harmonic Constituents with the table above (Make sure to use meters and UTC time for the CO-OPS estimated harmonic coefficients!). You will see that the estimates for the Amplitude and Phase are quite similar for the most significant tides. However, given that there are no significant differences in the numerical approaches how we estimated the coefficients here and how CO-OPS estimates the coefficients they are still significant. \n",
    "\n",
    "- q_2_2_2_1: Explain what the most important reason is that the estimates are slightly different knowing that:\n",
    "    - 1. We can get our estimates significantly closer to the CO-OPS estimates by downloading more data\n",
    "    - 2. One hour intervals are small enough to easily meet the Nyquist sampling intervals for all constituents.\n",
    "    - 3. Changing the `method` and `conf_int` in the call to `utide.solve()` will result only in marginal differences.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2\n",
    "q_2_2_2_1 = 'The most important reason for the differences in the estimated constituents is ...'\n",
    "\n",
    "aq_2_2_2_1 = 'The most important reason for the differences in the estimated constituents is the length of the observation record'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2.3 Tidal Reconstruction and Prediction\n",
    "\n",
    "**Tidal reconstruction** is done through the summation of all available harmonic constituents. Tidal reconstrcution is the generation of a tidal signal matching the observation period. Tidal reconstruction thus allows for the removal of the tidal influence on the water level leading to a residual water level signal that is due to non-tidal influences. This residual signal may then be further analyzed. Tidal reconstruction is a powerful tool for physical oceanography and many other applications.\n",
    "\n",
    "**Tidal prediction** uses the same harmonic constituents, but to predict water levels before (hind-casting) are after (fore-casting) the observation period. Tidal prediction is an invaluable tool for navigation, as well as many other activities in the near coastal zones. \n",
    "\n",
    "The `utide.reconstruct()` function is used for both reconstruction and prediction; the only difference being the epoch for which the function is used.\n",
    "\n",
    "Though the summation may be thought of the summation of a series of cosines, as shown on the [CO-OPS web page describing harmonic constituents](https://tidesandcurrents.noaa.gov/about_harmonic_constituents.html) it is implemented in the form of weighed least squares solution in `Utide`, where the weights are determined by the uncertainty levels associated to the tidal harmonic coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tidal predictions for the same epochs for which observation data where gathered\n",
    "\n",
    "tide_jl = utide.reconstruct(obs_jl.index, coef_jl, verbose=True)\n",
    "res_jl =  obs_jl.anomaly - tide_jl.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above then results in the reconstruction of the tides at the Jackson Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(figsize=(17, 8), nrows=3, sharey=True, sharex=True)\n",
    "\n",
    "ax0.plot(obs_jl.index, obs_jl.anomaly, label=\"Observations\", color=\"C0\")\n",
    "ax0.set_title('Water Level [m]')\n",
    "ax0.set(ylabel='Water Level [m]')\n",
    "ax1.plot(obs_jl.index, tide_jl.h, label=\"Prediction\", color=\"C1\")\n",
    "ax1.set_title('Predicted Tide [m]')\n",
    "ax1.set(ylabel='Predicted Tide [m]')\n",
    "ax2.plot(obs_jl.index, res_jl, label=\"Residual\", color=\"C2\")\n",
    "ax2.set_title('(Water Level - Predicted Tide Residuals) [m]')\n",
    "ax2.set(ylabel='Residual [m]')\n",
    "ax2.set(xlabel='Date [UTC]')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.suptitle('Jackson Laboratory CCOM/JHC Tide Gauge Water Level, Complete Record')\n",
    "# plt.ylabel('Water Level [m]')\n",
    "plt.gcf().autofmt_xdate()\n",
    "fig.legend(ncol=3, loc=\"lower center\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above results in the visualization of the observed water level, the reconstructed tidal signal contained therein, and finally, the residual signal due to non-tidal processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the 5 most significant constituents at Fort Point\n",
    "\n",
    "pred_0_jl = utide.reconstruct(obs_jl.index, coef_jl,constit=coef_jl.name[0:1])\n",
    "pred_1_jl = utide.reconstruct(obs_jl.index, coef_jl,constit=coef_jl.name[1:2])\n",
    "pred_2_jl = utide.reconstruct(obs_jl.index, coef_jl,constit=coef_jl.name[2:3])\n",
    "pred_3_jl = utide.reconstruct(obs_jl.index, coef_jl,constit=coef_jl.name[3:4])\n",
    "pred_4_jl = utide.reconstruct(obs_jl.index, coef_jl,constit=coef_jl.name[4:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 5 most significant tidal constituents and their combined sum\n",
    "\n",
    "pred_5c_jl = pred_0_jl.h+pred_1_jl.h+pred_2_jl.h+pred_3_jl.h+pred_4_jl.h\n",
    "\n",
    "fig, (ax0, ax1, ax2,ax3,ax4, ax5) = plt.subplots(nrows=6, sharey=True, sharex=True,figsize=(10, 10))\n",
    "ax0.plot(obs_jl.index[0:200],pred_0_jl.h[0:200], label=coef_jl.name[0:1][0], color=\"C0\")\n",
    "ax0.set(ylabel=coef_jl.name[0:1][0]+' [m]')\n",
    "ax1.plot(obs_jl.index[0:200],pred_1_jl.h[0:200], label=coef_jl.name[1:2][0], color=\"C1\")\n",
    "ax1.set(ylabel=coef_jl.name[1:2][0]+' [m]')\n",
    "ax2.plot(obs_jl.index[0:200],pred_2_jl.h[0:200], label=coef_jl.name[2:3][0], color=\"C2\")\n",
    "ax2.set(ylabel=coef_jl.name[2:3][0]+' [m]')\n",
    "ax3.plot(obs_jl.index[0:200],pred_3_jl.h[0:200], label=coef_jl.name[3:4][0], color=\"C3\")\n",
    "ax3.set(ylabel=coef_jl.name[3:4][0]+' [m]')\n",
    "ax4.plot(obs_jl.index[0:200],pred_4_jl.h[0:200], label=coef_jl.name[4:5][0], color=\"C4\")\n",
    "ax4.set(ylabel=coef_jl.name[4:5][0]+' [m]')\n",
    "ax5.plot(obs_jl.index[0:200],pred_5c_jl[0:200], label='Combined', color=\"C5\")\n",
    "ax5.set(ylabel='Combined [m]')\n",
    "\n",
    "fig.legend(ncol=6, loc=\"lower center\")\n",
    "plt.suptitle('Jackson Laboratory: 5 Most Significant Constituents')\n",
    "plt.xlabel('Date')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output: \n",
    "<img align=\"center\" width=\"40%\" style=\"padding-right:10px;\" src=\"Images/2_3_JL_reconstruction_5.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two code cells above illustrate the tidal reconstruction for individual harmonic signals, and their summation to create a complex tidal signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Tidal Reconstruction at Fort Point\n",
    "\n",
    "Complete the code below to reconstruct the tides at Fort Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tidal predictions for the same epochs for which observation data where gathered\n",
    "\n",
    "tide_fp = utide.reconstruct(...)\n",
    "res_fp =  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code cell below to visualize the observed data, derived tide model, and residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(figsize=(17, 8), nrows=3, sharey=True, sharex=True)\n",
    "\n",
    "ax0.plot(obs_fp.index, obs_fp.anomaly, label=\"Observations\", color=\"C0\")\n",
    "ax0.set_title('Water Level [m]')\n",
    "ax0.set(ylabel='Water Level [m]')\n",
    "ax1.plot(obs_fp.index, tide_fp.h, label=\"Prediction\", color=\"C1\")\n",
    "ax1.set_title('Predicted Tide [m]')\n",
    "ax1.set(ylabel='Predicted Tide [m]')\n",
    "ax2.plot(obs_fp.index, res_fp, label=\"Residual\", color=\"C2\")\n",
    "ax2.set_title('(Water Level - Predicted Tide Residuals) [m]')\n",
    "ax2.set(ylabel='Residual [m]')\n",
    "ax2.set(xlabel='Date [UTC]')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.suptitle('Fort Point Tide Gauge Water Level, Complete Record')\n",
    "# plt.ylabel('Water Level [m]')\n",
    "plt.gcf().autofmt_xdate()\n",
    "fig.legend(ncol=3, loc=\"lower center\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Residual Water Level Differences\n",
    "\n",
    "The residual water levels are a topic of interest for many. They indicate the water level variations that are due to non-tidal influences. Due to the fact that they are obtained by removing a modeled tide signal from observed water levels there are inherent errors associated to the residuals.\n",
    "\n",
    "Here we will have a quick look at some of the residual properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0) = plt.subplots(figsize=(17, 6), nrows=1, sharey=True, sharex=False)\n",
    "ax0.plot(obs_fp.index[day_1_fp_indexes], res_fp[day_1_fp_indexes], label=\"Fort Point\", color=\"C1\")\n",
    "ax0.plot(obs_jl.index[day_1_jl_indexes], res_jl[day_1_jl_indexes], label=\"Jackson Lab\", color=\"C2\")\n",
    "\n",
    "plt.title('Water Level Residuals: '+str(day_1_date))\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date')\n",
    "fig.legend(ncol=3, loc=\"lower center\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "fig, (ax1) = plt.subplots(figsize=(17, 6), nrows=1, sharey=True, sharex=False)\n",
    "ax1.plot(obs_fp.index, res_fp, label=\"Fort Point\", color=\"C1\")\n",
    "ax1.plot(obs_jl.index, res_jl, label=\"Jackson Lab\", color=\"C2\")\n",
    "# ax0.set_title('(Water Level - Predicted Tide Residuals) [m]')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('Water Level Residuals: Complete Record')\n",
    "plt.ylabel('Water Level [m]')\n",
    "plt.xlabel('Date')\n",
    "fig.legend(ncol=3, loc=\"lower center\")\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the residual signals at either station shown above it is clear that they are not primarily due to instrument noise.\n",
    "\n",
    "- q3_1: Explain why the plots above show that the residuals are due primarily to factors other than instrument noise.\n",
    "- q3_2: Explain why you would not expect to see the same time delay in the residual signals as you would for the tidal signal.\n",
    "- q3_3: The residuals for the Fort Point water level data are noisier than the Jackson Laboratory data, explain why we would expect this given how the data was sampled.\n",
    "\n",
    "BONUS (No points): Given what you learned about analyzing tides can you think of a method that can be used to convince you that most or all tidal energy has been accounted for in the residual signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_1 = 'The observed residuals are due primarily to factors other than instrument noise ...'\n",
    "q3_2 = 'The residuals are ...'\n",
    "q3_3 = 'The primary reason the residuals are noisier is ...'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img align=\"left\" width=\"6%\" style=\"padding-right:10px; padding-top:10px;\" src=\"Images/refs.png\">\n",
    "\n",
    "## <br>Useful References<br>\n",
    "\n",
    "- [About NOAA CO-OPS](https://tidesandcurrents.noaa.gov/about_us.html)\n",
    "- [NOAA CO-OPS tides and currents map page](https://tidesandcurrents.noaa.gov/map/)\n",
    "- [NOAA Special Publication NOS CO-OPS 3](Documents/Tidal_Analysis_and_Predictions.pdf): Book on tidal analysis and Prediction\n",
    "- [American Mathematical Society (AMA) web page](https://www.ams.org/home/page)\n",
    "- [AMA tide machine animation](http://www.ams.org/publicoutreach/feature-column/fcarc-tidesiii3)\n",
    "- [Utide web page](http://www.po.gso.uri.edu/~codiga/utide/utide.htm)\n",
    "- [T_TIDE documentation](Documents/t_tide.pdf)\n",
    "- [Original t_tide algoritm development by Mike Foreman](Documents/foremanREP1978.pdf)\n",
    "- [Python Data Analysis Library (pandas)](https://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Tidal Analysis Using Utide v2.0.0, Semme J. Dijkstra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
